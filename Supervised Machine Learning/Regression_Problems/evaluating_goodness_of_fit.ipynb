{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4rxw94wkzsf"
      },
      "source": [
        "So far, you've learned how to build linear regression models and interpret estimated coefficients. In this lesson, you'll explore how to evaluate model performance in the training phase. Recall that there are two contexts where performance matters: with the training set and with a test set. When you evaluate performance related to the training set, that enables you to talk about how well your model explains the information in the target variable. And evaluating a test set's performance tells you how well your model will perform when it's given previously unseen observations.\n",
        "\n",
        "In this lesson, you'll go over concepts like *F-tests* and *R-squared*. F-tests allow you to compare your model to a reduced model with no features. R-squared (and *adjusted R-squared*, which is a variant of R-squared) values tell you how well the model accounts for variance in the target.\n",
        "\n",
        "After that, you'll see how to compare different models in terms of their explanatory power. You'll learn how to read the *Akaike information criterion* and the *Bayesian information criterion* for this purpose.\n",
        "\n",
        "## Key topics\n",
        "\n",
        "* Training and test data\n",
        "* Evaluating training performance\n",
        "* F-tests\n",
        "* Degrees of freedom\n",
        "* R-squared\n",
        "* Akaike information criterion\n",
        "* Bayesian information criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yRFNWcRkzsj"
      },
      "source": [
        "## Is your model better than an \"empty\" model?\n",
        "\n",
        "When evaluating your model, you first need to ask whether your model contributes anything to the explanation of the outcome variable. In other words, you need to determine whether or not your features explain variance in the outcome. If they don't, you could drop your features altogether, and the resulting \"empty\" model would perform equally well—which is to say, not very well!\n",
        "\n",
        "For this purpose, use an *F-test*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vmUz8ckkzsk"
      },
      "source": [
        "###  F-tests\n",
        "\n",
        "F-tests can be calculated in different ways, depending on the situation. But in general, they represent the ratio between a model's unexplained variance compared to a reduced model. Here, the reduced model is a model with no features, meaning that all variance in the outcome is unexplained. For a linear regression model with two parameters $y=\\alpha+\\beta x$, the F-test is built from these pieces:\n",
        "\n",
        "| Name | Equation |\n",
        "| :-- | :-- |\n",
        "| Unexplained model variance | $$SSE_F=\\sum(y_i-\\hat{y}_i)^2$$ |\n",
        "| Unexplained variance in reduced model | $$SSE_R=Var_y = \\sum(y_i-\\bar{y})^2$$ |\n",
        "| Number of parameters in the model | $$p_F = 2$$ $$(\\alpha \\text{ and } \\beta)$$ |\n",
        "| Number of parameters in the reduced model | $$p_R = 1 $$ $$(\\alpha)$$ |\n",
        "| Number of observations | $$n$$ |\n",
        "| Degrees of freedom of $SSE_F$ | $$df_F = n - p_F$$ |\n",
        "| Degrees of freedom of $SSE_R$ | $$df_R = n - p_R$$ |\n",
        "\n",
        "These pieces come together to give you the full equation for the F-test:\n",
        "\n",
        "$$F=\\dfrac{SSE_F-SSE_R}{df_F-df_R}÷\\dfrac{SSE_F}{df_F}$$\n",
        "\n",
        "This equation introduces some new terminology. *Degrees of freedom* quantifies the amount of information \"left over\" to estimate variability after all parameters are estimated.\n",
        "\n",
        "To gain some intuition on what degrees of freedom are, consider the following scenario: You calculate the mean of three numbers and determine that it is `5`. If you know of the value of two of the numbers (for example, imagine that you know that two of the numbers are `3` and `5`), then the value of the third response is effectively \"locked in\" and has no freedom to vary (continuing from the previous example, the third value must be `7`). In this scenario, the mean is a calculated *parameter* or *statistic* of the data, and the process of estimating this parameter restricts the freedom of one of the observations to vary, leaving you with two \"independent values\"—which means that there are two degrees of freedom.\n",
        "\n",
        "This idea can be extended to linear regression models, which usually involve estimating more than one parameter. Imagine a dataset with two observations, with each observation consisting of one feature $x$ and the corresponding response $y$. If a regression model $y=\\alpha + \\beta x$ is built on this dataset, basic algebra allows you to calculate the values of $\\alpha$ and $\\beta$ corresponding to the unique line that passes through the two points. The response values have no freedom to vary; they are effectively \"locked in\" by the values of $\\alpha$, $\\beta$, and their associated values for $x$. If you have more than two observations, you are no longer guaranteed to have a model that perfectly fits the data. Then each observation beyond the second has the freedom to vary. For example, if you train the same model on 10 observations instead of 2, 8 of the observations would be free to vary.\n",
        "\n",
        "When building linear regression models, the data's freedom to vary ultimately depends on two things: the number of observations and the number of parameters estimated by the model. This forms the basis for the following terminology when performing statistical tests on linear regression models with $k$ parameters trained on $n$ observations:  \n",
        "  \n",
        "$df_{model}$ (model degrees of freedom): The number of features in the model, or alternatively $k - 1$ (minus 1 for the intercept term)  \n",
        "$df_{error}$ (error degrees of freedom): $n-df_{model}$  \n",
        "$df_{total}$ (total degrees of freedom): $df_{model}+df_{error}$, or alternatively, $n-1$  \n",
        "\n",
        "The higher $df_{error}$ is (the more data you have that is free to vary), the more statistical power you have when it comes to things like performing hypothesis tests on the coefficients estimated by your linear regression models.\n",
        "\n",
        "The F-test's null hypothesis states that the model is indistinguishable from the reduced model, which means that the features contribute nothing to the explanation of the target variable. Instead of reading the F-statistic, it's easier to read its associated p-value. The lower the p-value, the better your model. Namely, if the p-value of the F-test for your model is less than or equal to `0.1` (or even less than or equal to `0.05`), you can say that your model is useful and contributes something that is statistically significant in the explanation of the target.\n",
        "\n",
        "Now, calculate the F-statistic of your medical costs model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Puwulg4qkzsm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sqlalchemy import create_engine, text\n",
        "\n",
        "# Display preferences\n",
        "%matplotlib inline\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "zChLy_krkzso",
        "outputId": "8ac69b02-a676-4dde-b474-50dd8c6b5774"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age     sex    bmi  children smoker     region   charges\n",
              "0   19  female 27.900         0    yes  southwest 16884.900\n",
              "1   18    male 33.770         1     no  southeast  1725.550\n",
              "2   28    male 33.000         3     no  southeast  4449.460\n",
              "3   33    male 22.705         0     no  northwest 21984.500\n",
              "4   32    male 28.880         0     no  northwest  3866.860\n",
              "5   31  female 25.740         0     no  southeast  3756.620\n",
              "6   46  female 33.440         1     no  southeast  8240.590\n",
              "7   37  female 27.740         3     no  northwest  7281.510\n",
              "8   37    male 29.830         2     no  northeast  6406.410\n",
              "9   60  female 25.840         0     no  northwest 28923.100"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f89e4fea-c61e-4a6b-bc2a-685f973ca378\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>children</th>\n",
              "      <th>smoker</th>\n",
              "      <th>region</th>\n",
              "      <th>charges</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>female</td>\n",
              "      <td>27.900</td>\n",
              "      <td>0</td>\n",
              "      <td>yes</td>\n",
              "      <td>southwest</td>\n",
              "      <td>16884.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>male</td>\n",
              "      <td>33.770</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>1725.550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28</td>\n",
              "      <td>male</td>\n",
              "      <td>33.000</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>4449.460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33</td>\n",
              "      <td>male</td>\n",
              "      <td>22.705</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>21984.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>32</td>\n",
              "      <td>male</td>\n",
              "      <td>28.880</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>3866.860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>31</td>\n",
              "      <td>female</td>\n",
              "      <td>25.740</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>3756.620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>46</td>\n",
              "      <td>female</td>\n",
              "      <td>33.440</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>8240.590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>37</td>\n",
              "      <td>female</td>\n",
              "      <td>27.740</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>7281.510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>37</td>\n",
              "      <td>male</td>\n",
              "      <td>29.830</td>\n",
              "      <td>2</td>\n",
              "      <td>no</td>\n",
              "      <td>northeast</td>\n",
              "      <td>6406.410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>60</td>\n",
              "      <td>female</td>\n",
              "      <td>25.840</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>28923.100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f89e4fea-c61e-4a6b-bc2a-685f973ca378')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f89e4fea-c61e-4a6b-bc2a-685f973ca378 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f89e4fea-c61e-4a6b-bc2a-685f973ca378');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a64fbe66-ed80-4741-b866-dd3cf3f19f27\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a64fbe66-ed80-4741-b866-dd3cf3f19f27')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a64fbe66-ed80-4741-b866-dd3cf3f19f27 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "postgres_user = 'dsbc_student'\n",
        "postgres_pw = '7*.8G9QH21'\n",
        "postgres_host = '142.93.121.174'\n",
        "postgres_port = '5432'\n",
        "postgres_db = 'medicalcosts'\n",
        "\n",
        "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(\n",
        "    postgres_user, postgres_pw, postgres_host, postgres_port, postgres_db))\n",
        "\n",
        "# insurance_df = pd.read_sql_query('select * from medicalcosts',con=engine)\n",
        "with engine.begin() as conn:\n",
        "    query = text(\"\"\"SELECT * FROM medicalcosts\"\"\")\n",
        "    insurance_df = pd.read_sql_query(query, conn)\n",
        "\n",
        "\n",
        "# No need for an open connection, because you're only doing a single query\n",
        "engine.dispose()\n",
        "\n",
        "insurance_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OBbQd9Wkzsq",
        "outputId": "1ef0d4c5-188b-43d8-f9ef-9c31b6b3952d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                charges   R-squared:                       0.747\n",
            "Model:                            OLS   Adj. R-squared:                  0.747\n",
            "Method:                 Least Squares   F-statistic:                     986.5\n",
            "Date:                Sat, 11 Nov 2023   Prob (F-statistic):               0.00\n",
            "Time:                        01:32:40   Log-Likelihood:                -13557.\n",
            "No. Observations:                1338   AIC:                         2.712e+04\n",
            "Df Residuals:                    1333   BIC:                         2.715e+04\n",
            "Df Model:                           4                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const      -1.163e+04    947.267    -12.281      0.000   -1.35e+04   -9775.195\n",
            "is_male     -109.0414    334.665     -0.326      0.745    -765.568     547.486\n",
            "is_smoker   2.383e+04    414.187     57.544      0.000     2.3e+04    2.46e+04\n",
            "age          259.4531     11.942     21.727      0.000     236.027     282.880\n",
            "bmi          323.0510     27.529     11.735      0.000     269.046     377.056\n",
            "==============================================================================\n",
            "Omnibus:                      299.394   Durbin-Watson:                   2.076\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              708.640\n",
            "Skew:                           1.212   Prob(JB):                    1.32e-154\n",
            "Kurtosis:                       5.614   Cond. No.                         292.\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ],
      "source": [
        "insurance_df[\"is_male\"] = pd.get_dummies(insurance_df.sex, drop_first=True)\n",
        "insurance_df[\"is_smoker\"] = pd.get_dummies(insurance_df.smoker, drop_first=True)\n",
        "\n",
        "# `Y` is the target variable\n",
        "Y = insurance_df['charges']\n",
        "\n",
        "# `X` is the feature set\n",
        "X = insurance_df[['is_male','is_smoker', 'age', 'bmi']]\n",
        "\n",
        "# Add a constant to the model because it's best practice\n",
        "# to do so every time!\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit an OLS model using statsmodels\n",
        "results = sm.OLS(Y, X).fit()\n",
        "\n",
        "# Print the summary results\n",
        "print(results.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3Z4HpUFkzsr"
      },
      "source": [
        "This model's F-statistic is `986.5`, and the associated p-value is very close to zero. This means that your features add some information to the reduced model, and your model is useful in explaining `charges`.\n",
        "\n",
        "However, F-tests don't quantify how much information your model contributes. That requires R-squared, which you'll learn about next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKSJOrypkzss"
      },
      "source": [
        "## Quantifying a model's performance on the training set\n",
        "\n",
        "R-squared is probably the most common measure of goodness of fit in a linear regression model. It is a proportion (between `0` and `1`) that expresses how much variance in the outcome variable is explained by the explanatory variables in the model. Generally speaking, higher $R^2$ values are better to a point. A low $R^2$ indicates that your model isn't explaining much information about the outcome, which means that it will not give very good predictions. But a very high $R^2$ is a warning sign of overfitting. No dataset is a perfect representation of reality, so a model that perfectly fits your data ($R^2$ of `1` or close to `1`) is likely to be biased by quirks in the data and will perform less well on the test set.\n",
        "\n",
        "In the regression summary table above, you can see that your medical costs model's R-squared value is `0.747`. This means that your model explains 74.7% of the variance in the charges, leaving 25.3% unexplained. You can conclude that there's still room for improvement. Now, fit the model in the previous lesson again, where you included the interaction of `BMI` and the `is_smoking` dummy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROas-opvkzst",
        "outputId": "22f7bb5d-adca-4f41-bd6f-069fc917062a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                charges   R-squared:                       0.837\n",
            "Model:                            OLS   Adj. R-squared:                  0.836\n",
            "Method:                 Least Squares   F-statistic:                     1365.\n",
            "Date:                Sat, 11 Nov 2023   Prob (F-statistic):               0.00\n",
            "Time:                        01:32:40   Log-Likelihood:                -13265.\n",
            "No. Observations:                1338   AIC:                         2.654e+04\n",
            "Df Residuals:                    1332   BIC:                         2.657e+04\n",
            "Df Model:                           5                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=================================================================================\n",
            "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
            "---------------------------------------------------------------------------------\n",
            "const         -2071.0750    840.644     -2.464      0.014   -3720.206    -421.944\n",
            "is_male        -473.4954    269.612     -1.756      0.079   -1002.406      55.415\n",
            "is_smoker     -2.019e+04   1666.492    -12.117      0.000   -2.35e+04   -1.69e+04\n",
            "age             266.3723      9.612     27.713      0.000     247.516     285.228\n",
            "bmi               7.9686     25.044      0.318      0.750     -41.160      57.098\n",
            "bmi_is_smoker  1435.6081     53.242     26.964      0.000    1331.160    1540.056\n",
            "==============================================================================\n",
            "Omnibus:                      710.004   Durbin-Watson:                   2.059\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4260.532\n",
            "Skew:                           2.491   Prob(JB):                         0.00\n",
            "Kurtosis:                      10.183   Cond. No.                         661.\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ],
      "source": [
        "# `Y` is the target variable\n",
        "Y = insurance_df['charges']\n",
        "\n",
        "# This is the interaction between BMI and smoking\n",
        "insurance_df[\"bmi_is_smoker\"] = insurance_df.bmi * insurance_df.is_smoker\n",
        "\n",
        "# `X` is the feature set\n",
        "X = insurance_df[['is_male','is_smoker', 'age', 'bmi', \"bmi_is_smoker\"]]\n",
        "\n",
        "# Add a constant to the model because it's best practice\n",
        "# to do so every time!\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit an OLS model using statsmodels\n",
        "results = sm.OLS(Y, X).fit()\n",
        "\n",
        "# Print the summary results\n",
        "print(results.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABFc_nY3kzsu"
      },
      "source": [
        "The R-squared of this model is `0.837`, which is higher than your previous model's R-squared. This improvement indicates that the interaction of `BMI` and `is_smoker` explains some previously unexplained variance in `charges`.\n",
        "\n",
        "As mentioned before, high R-squared values are generally desirable. But, in some cases, very high R-squared values indicate some potential problems with a model. Specifically, it could mean the following:\n",
        "\n",
        "* A very high R-squared value may be a sign of overfitting. If your model is too complex for the data, then it may overfit the training set and do a poor job on the test set. That said, there isn't an agreed-upon R-squared threshold to detect overfitting. Instead, you need to compare the model's performance on test versus training data. If your model performs significantly worse on the test set than the training set, you should suspect overfitting. You'll explore how to evaluate linear regression models on the test set in the next lesson.\n",
        "\n",
        "* R-squared is an inherently biased estimate of the performance, in the sense that the more explanatory variables are added to the model, the higher R-squared values you get. This is true even when you include irrelevant variables like noises or random data. To mitigate this problem, data scientists usually use a metric called *adjusted R-squared* instead of *R-squared*. Adjusted R-squared does the same job as R-squared, but it is adjusted according to the number of features included in the model. Hence, it's always safer to look at the adjusted R-squared value instead of R-squared value.\n",
        "\n",
        "**A note on negative R-squared values:** It is possible to get negative R-squared values for some models. In general, if a model is weaker than a straight horizontal line, then the R-squared value becomes negative. This usually happens when a constant is not included in the model. If you get a negative value for R-squared, that means that your model explains the target very poorly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ko9tmZTkzsv"
      },
      "source": [
        "## Comparing different models\n",
        "\n",
        "Comparing different models and choosing the best one is an essential practice in data science. Often, you'll try several models and evaluate their performance on a test set in order to determine the top-performing model. However, *inference* is also a critical task when it comes to linear regression models. Unlike testing the predictive power, in inference, you care about the explanatory power of your models.\n",
        "\n",
        "Throughout this lesson, you've seen that you can measure the performance of your models on the training set using F-test or R-squared. Hence, both F-test and R-squared can be used to compare different models. Unfortunately, the two metrics suffer from some drawbacks that make them inappropriate to use in certain situations.\n",
        "\n",
        "Here, you'll briefly explore how you can use F-tests and R-squared to compare models. Then, you'll learn about information criteria that you can also use to compare different models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni3OF9Kgkzsw"
      },
      "source": [
        "### Using F-tests for model comparison\n",
        "\n",
        "You can use an F-test to compare two models if one of them is nested within the other. That is, if the feature set in a model is a subset of the feature set of the other, then you can use an F-test. In this case, you say that the model with the higher F-statistic is superior to the other model.\n",
        "\n",
        "However, if models are not nested, then using an F-test may be misleading. F-tests are quite sensitive to the normality of the error terms. If errors are not normally distributed, you should try other methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpEElaL8kzsx"
      },
      "source": [
        "### Using R-squared for model comparison\n",
        "\n",
        "R-squared can also be used. You already saw that R-squared is biased, as it tends to increase with the number of explanatory variables. So, instead of R-squared, you can use adjusted R-squared. The higher the adjusted R-squared, the better the model explains the target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edG0kezjkzsy"
      },
      "source": [
        "### Using information criteria\n",
        "\n",
        "Using information criteria is also a common way of comparing different models and selecting the best one. Here, you'll learn about two information criteria: *Akaike information criterion* (AIC) and *Bayesian information criterion* (BIC). Both take into consideration the sum of the squared errors (SSE), the sample size, and the number of parameters.\n",
        "\n",
        "The formula for AIC is as follows:\n",
        "\n",
        "$$nln(SSE)−nln(n)+2p$$\n",
        "\n",
        "\n",
        "And the formula for BIC is as follows:\n",
        "\n",
        "$$nln(SSE)−nln(n)+pln(n)$$\n",
        "\n",
        "In both of these formulas, $n$ represents the sample size, $p$ represents the number of regression coefficients in the model (including the constant), and $ln$ stands for the natural logarithm.\n",
        "\n",
        "For both AIC and BIC, lower values indicate better models. So you should choose the model with the lowest AIC or BIC value. Although you can use either of the two criteria, AIC is often criticized for its tendency to overfit. In contrast, BIC penalizes the number of parameters more severely than AIC, so it favors more parsimonious models (that is, models with fewer parameters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaAdPvXskzsy"
      },
      "source": [
        "## Which medical costs model is better?\n",
        "\n",
        "The statmodels `summary()` function provides all of the above metrics. Take a look at these metrics in the tables above. For your first model, R-squared is `0.747`, adjusted R-squared is `0.747`, the F-statistic is `986.5`, AIC is `27.120`, and BIC is `27.150`. For your second model, R-squared is `0.837`, adjusted R-squared is `0.836`, the F-statistic is `1365`, AIC is `26.540`, and BIC is `26.570`. According to all of the metrics, the second model seems better than the first one."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}