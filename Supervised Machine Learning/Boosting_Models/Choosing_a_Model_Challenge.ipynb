{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Choosing a model: Challenge\n",
        "For each item below, identify which supervised learning method (or methods) would be best for addressing that particular problem. Write your thoughts in the space below, and explain your reasoning.\n"
      ],
      "metadata": {
        "id": "1Hd0VARzzrnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. Predict the running times of prospective Olympic sprinters given the data from the last 20 Olympics.\n",
        "\n",
        "Answer: As a first pass, it would be good idea to try the k-nearest neighbor (KNN) algorithm. The times of the twenty fastest runners from 2023 should be close to the twenty fastest runners in 2022. A [2022 study](https://www.thieme-connect.com/products/ejournals/pdf/10.1055/a-1993-2371.pdf) was done comparing the KNN algorithm to an artificial neural network (ANN). The researchers found that the KNN algorithm outperformed ANN with a mean absolute error of 2\\% or 4\\% versus 5\\% or 6\\%.\n",
        "\n",
        "As runners are [getting faster over time](https://www.sportsrec.com/557769-why-were-running-faster-than-ever.html), I would also consider the random forest algorithm. Random forest is good at predicting values from a range of values that it has previously seen. We could also use a RNN (more specifically, LSTM which measures time series data over a time period of months or years).\n",
        "\n",
        "Methods to try:\n",
        "\n",
        "- KNN\n",
        "- Random Forest\n",
        "- RNN (LSTM)"
      ],
      "metadata": {
        "id": "poZYQM0Uz08O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. You have more features (columns) than rows in your dataset.\n",
        "\n",
        "Answer: The most obvious technique would be using a dimensionality reduction technique such as PCA. We likely don't need to use all the features in our dataset and could use PCA to find the most important features.\n",
        "\n",
        "Also, as discussed [here](https://medium.com/@jennifer.zzz/more-features-than-data-points-in-linear-regression-5bcabba6883e), too many variables can lead to overfitting.  We can use regularization to reduce overfitting. For this, we can use [L1 Lasso Regression or L2 Ridge Regression](https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b).\n",
        "\n",
        "Methods to try:\n",
        "\n",
        "- PCA\n",
        "- L1 Lasso or L2 Ridge Regression"
      ],
      "metadata": {
        "id": "VD00MsDQz7Od"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. Identify the most important characteristic for predicting the likelihood of being jailed before age 20.\n",
        "\n",
        "Answer: As a first pass, we could use Gradient boosting to see feature importance.\n",
        "\n",
        "We could also treat this as a binary classification problem by which going to jail or not is a binary outcome. We could use one-hot encoding to map categorial features to a binary variable containing 0 or 1. Afterwards, we would apply the ID3 algorithm to generate a decision tree which would identify the most important features. This sort of classification problem would naturally fall under logistic regression.\n",
        "\n",
        "I would also imagine that a Support Vector Machine (SVC) classifier would allow us to find a hyperplane that divides the two classes (going to jail or continuing with normal life).\n",
        "\n",
        "Methods to try:\n",
        "\n",
        "- Gradient boosting\n",
        "- Decision Tree\n",
        "- Logistic Regression\n",
        "- Support Vector Machine"
      ],
      "metadata": {
        "id": "8YK7QQcKz7Zh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. Implement a filter to highlight emails that might be important to the recipient.\n",
        "\n",
        "Answer: I have used the [Naive Bayes algorithm](https://github.com/matthewshawnkehoe/Data-Analysis/blob/main/src/Building_a_Spam_Filter_with_Naive_Bayes.ipynb) to prevent spam filtering. This will get rid of spam emails that aren't useful to the end user.\n",
        "\n",
        "Methods to try:\n",
        "- Naive Bayes"
      ],
      "metadata": {
        "id": "0g4ExaBoz7ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. You have more than 1,000 features.\n",
        "\n",
        "Answer: As before, one can use Ridge or Lasso Regression to reduce the number of features. We could also use Principal Component Analysis to combine features with similar variance, allowing us to reduce the set of features to something more manageable.\n",
        "\n",
        "A third idea is using Random Forest. Each individual tree won't necessarily look at all of the features, but together the forest will likely cover them all, and collectively, they will be able to sort through and find the most important features.\n",
        "\n",
        "Methods to try:\n",
        "- PCA\n",
        "- L1 Lasso or L2 Ridge Regression\n",
        "- Random Forest"
      ],
      "metadata": {
        "id": "6xWYK8A4z7io"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6. Predict whether someone who adds items to their cart on a website will purchase the items.\n",
        "\n",
        "Answer: This is another binary classification problem. As such, a good first model to try is the Random Forest classifier.\n",
        "\n",
        "We could also use the KNN algorithm or Support Vector Machine (SVC) classifier. The Support Vector Classifier should allow us to find a hyperplane that divides the two classes (will purchase vs. won't purchase). Similarly, KNN would likely show us that past purchasers have some similar attributes that group them together.\n",
        "\n",
        "Methods to try:\n",
        "- KNN\n",
        "- Support Vector Machine\n",
        "- Random Forest\n"
      ],
      "metadata": {
        "id": "u7GkEHdw0K3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7. Your dataset dimensions are 982400x500.\n",
        "\n",
        "Answer: Similar to before, our dataset has a lot of records and 500 features. Therefore, we could try Ridge or Lasso regression, or PCA, to reduce the number of features to something more manageable.\n",
        "\n",
        "Methods to try:\n",
        "\n",
        "- PCA\n",
        "- L1 Lasso or L2 Ridge Regression"
      ],
      "metadata": {
        "id": "rxPOhE8J0K9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8. Identify faces in an image.\n",
        "\n",
        "Answer: A lot of research in deep learning has been [applied to facial recognition problems](https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/). I would first try a convnet (CNN).\n",
        "\n",
        "Methods to try:\n",
        "\n",
        "- Deep Learning (state-of-the-art). To start, try a CNN\n",
        "- KNN\n",
        "- Random Forest"
      ],
      "metadata": {
        "id": "Y_yu9qG_0LCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q9. Predict which of three flavors of ice cream will be most popular with boys versus girls.\n",
        "\n",
        "Answer: This is a [multinomial classification problem](https://stats.oarc.ucla.edu/spss/output/multinomial-logistic-regression/). Once again, I would try KNN or random forest.\n",
        "\n",
        "Methods to try:\n",
        "- KNN\n",
        "- Random Forest\n"
      ],
      "metadata": {
        "id": "JgvH2hgv0VRq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B8-vJQaH0KV7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}