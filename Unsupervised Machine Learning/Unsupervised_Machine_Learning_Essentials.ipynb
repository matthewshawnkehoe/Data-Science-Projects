{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a23r3z1lUJY0"
      },
      "source": [
        "# Unsupervised Machine Learning\n",
        "\n",
        "Supervised learning focuses on creating algorithms that learn from labeled data. In unsupervised learning, we'll run algorithms on unlabeled data to derive useful patterns.\n",
        "\n",
        "**Key Terms**\n",
        "\n",
        "- **The curse of dimensionality:**\n",
        "    The exponential increase in the number of combinations for a linear increase in the number of dimensions\n",
        "- **Hard clustering:**\n",
        "    A type of clustering in which each data point is assigned to only one cluster\n",
        "- **Soft clustering:**\n",
        "    A type of clustering in which each data point is assigned a degree of membership for every cluster\n",
        "\n",
        "Unsupervised learning can be challenging, not because the algorithms are too difficult to grasp, but because the problems that unsupervised learning addresses are hard to formalize and evaluate. Most of human reasoning and learning operates in an unsupervised manner. In this section, we'll step into the exciting domain of unsupervised learning by exploring two main problems that it needs to tackle:\n",
        "\n",
        "1. Clustering\n",
        "2. Dimensionality reduction\n",
        "\n",
        "Although unsupervised learning includes other interesting topics, like anomaly detection, we'll focus on these two subjects.\n",
        "\n",
        "## What is clustering?\n",
        "\n",
        "Clustering is all about organizing individual examples or observations into groups, such that individuals in each group resemble fellow group members more than they do nongroup members. Clustering inherently requires the ability to measure similarity. For example, say that a management team gives us a customer dataset for their company. The management team asks us to help them organize customers into different groups in order to expedite customer service and drive reorganization strategies for the customer service department.\n",
        "\n",
        "Where to start? There's no need to overthink things, and we could start with a relatively straightforward grouping. We could separate the customers into different groups with respect to their city of origin, gender, income level, or all three of these variables.\n",
        "\n",
        "Grouping customers with respect to a single variable is easy. For example, we can separate the customers into two groups with respect to income."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B9B0OOnAwfj"
      },
      "source": [
        "![](https://images.ctfassets.net/c7lxnbtvvcxm/3JGWY79dK1wcyxBkm4Jf3F/6ff0a762ec63a393e761c32b3e4fa91d/customers.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2gz5pXS9CGV"
      },
      "source": [
        "From the graph above, we could assign the customers that lie above the red line to the high-income group and assign the rest to the low-income group. But what if we have hundreds of variables in your dataset, and all of them seem to be useful for the customer service efficiency? Assume, for example, that each of the hundred variables has two categories. If we group the customers using the technique above, we'll come up with $2^{100}$ groups—that's $1,267,650,600,228,229,401,496,703,205,376$ different groups!\n",
        "\n",
        "Clustering algorithms solve this problem by grouping observations (in this example, the customers) by taking into account all the variables at hand (in this example, 100 variables). Mathematically, the grouping is done in a high-dimensional space (in this example, 100 dimensions).\n",
        "\n",
        "Increasing data dimensionality doesn't require an increasing number of groups. Instead, clustering algorithms allow you to create an arbitrary number of groups. This means that we can apply clustering techniques to our customer data and ask the algorithm to come up with just four groups if our company plans to establish four different units inside the customer service department.\n",
        "\n",
        "## How many clusters?\n",
        "\n",
        "Many clustering algorithms require that we specify a desired number of groups as an input parameter.\n",
        "\n",
        "Take a look at an example. Imagine that we have a customer dataset with two variables: age and income. How many clusters do we think are appropriate?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zh4Vg3i9iO_"
      },
      "source": [
        "![](https://images.ctfassets.net/c7lxnbtvvcxm/1zj5z9S81bC8REfHygLFGl/9420f8f53236b88c87bdb2be7ae74c76/how_to_cluster_v2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gl20cYf9nU3"
      },
      "source": [
        "The graphs above show three ways that we could cluster this data:\n",
        "\n",
        "1.   Young versus old\n",
        "2.   Low-income versus high-income\n",
        "3.   Young and low-income versus young and high-income versus old and    low-income versus old and high-income\n",
        "\n",
        "Each of these clusterings may have its own advantages and disadvantages.\n",
        "\n",
        "In general, as the number of clusters increases, so does the similarity between the individuals within a cluster. This can be good to a point, but we don't want to end up with a large number of overfit clusters. Keep in mind that the limit case for clustering is that each individual case is its own cluster (which is really to say, there are no clusters). Just because an algorithm can find a certain number of clusters doesn't mean that those clusters are inherently meaningful.\n",
        "\n",
        "This gives us a hint as to why clustering is hard. In many cases, there isn't a single correct number of clusters. As we'll see in future analysis, for most clustering algorithms, the number of clusters is a hyperparameter that we need to tune.\n",
        "\n",
        "## Types of clustering\n",
        "\n",
        "**Cluster:** A set of data points that have been determined to be more similar to one another than to other data points\n",
        "\n",
        "There are two types of clustering:\n",
        "\n",
        "* In hard clustering, each data point is assigned to only one cluster.\n",
        "* In soft clustering, each data point is assigned a degree of membership for every cluster.\n",
        "\n",
        "In both types of clustering, a cluster is a set of data points that have been determined to be more similar to one another than to other data points. Depending on the clustering method that we use, we'll use different measures of similarity to analyze clusters.\n",
        "\n",
        "## What is dimensionality reduction?\n",
        "\n",
        "**Dimensionality reduction:** The process of reducing a higher-dimension dataset to a lower-dimension one, while striving to maintain as much information as possible\n",
        "\n",
        "Unsupervised learning often requires *dimensionality reduction*. In this context, *dimensions* refers to the number of features (variables) in the dataset.\n",
        "\n",
        "As a general rule, high-dimensional spaces contain at least as much information as low-dimensional spaces. That is, a dataset contains more information than a variant of that same dataset where the number of variables is reduced. This is why all dimensionality reduction techniques try to retain as much information as possible.\n",
        "\n",
        "To give us a sense of how we can represent a high-dimensional observation in a lower-dimensional space, the image below gives a familiar example. Take a look at these lower-dimensional representations of a three-dimensional cube:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13iQ8w4c-1bN"
      },
      "source": [
        "![](https://images.ctfassets.net/c7lxnbtvvcxm/3TXfSKZq4enQq2AEEvI57F/b1d4ff6aeca64ba6a1527e875ea463df/dimensions.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8qrS90j-3B8"
      },
      "source": [
        "In three-dimensional space, the cube has dimensions on the x-, y-, and z-axes. To represent this cube in two-dimensional space, we can use a square with two dimensions on the x- and y-axes. Similarly, we can represent a cube in one-dimensional space as a straight line on the x-axis.\n",
        "\n",
        "In this example, we retain quite a lot of information about the cube—even with a straight line. This is because the magnitudes of all the dimensions of a cube are the same. Now, think about a more complex three-dimensional object like a prism. In this case, we inevitably lose more information when we represent it in two- or one-dimensional space.\n",
        "\n",
        "Now, consider why dimensionality reduction is important. Say that we have a dataset that comprises millions of observations and thousands of features. In this case, dimensionality reduction can help with the following challenges:\n",
        "\n",
        "* Conducting exploratory data analysis on thousands of features is quite difficult and time consuming. It may take days, weeks, or even months to fully understand the relationships between the features.\n",
        "\n",
        "* Visualizations are one of the easiest ways to grasp data intuitively. But how can we visualize data in more than three dimensions?\n",
        "\n",
        "* Some machine learning algorithms suffer from the curse of dimensionality; as the number of features increases, the number of operations increases exponentially. Given a sufficient number of dimensions and records, algorithms can easily take days, weeks, or even months to run.\n",
        "\n",
        "* Last but not least, the more features we have, the more data we need to use to train our algorithms. Additional data collection is expensive and often not possible.\n",
        "\n",
        "This is why data scientists look to reduce dimensionality.\n",
        "\n",
        "We're already familiar with one approach to dimensionality reduction, principal component analysis (PCA), but there are many others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nPui619A9om"
      },
      "source": [
        "---\n",
        "# K-means\n",
        "\n",
        "We'll begin our exploration of clustering algorithms. Clustering is all about organizing individual examples or observations into groups, such that individuals in each group resemble fellow group members more than they do nongroup members. This requires the ability to measure similarity. We'll start by exploring the go-to, off-the-shelf algorithm for clustering data: the k-means algorithm.\n",
        "\n",
        "**Key Terms**\n",
        "\n",
        "- **Centroids:**\n",
        "    The best $k$ points that form the centers of each of the $k$ clusters in k-means\n",
        "- **Inertia:**\n",
        "    The loss function of the k-means algorithm\n",
        "\n",
        "## The k-means algorithm\n",
        "\n",
        "K-means is an iterative algorithm that eventually converges on a solution. The basic idea in k-means is to find the best $k$ points that form the centers of each of the $k$ clusters. These points are called centroids. The algorithm begins by choosing $k$ observations at random and making these observations the initial centroids. Then it iterates over the following two steps:\n",
        "\n",
        "1. Assign each data point to its nearest centroid.\n",
        "\n",
        "2. Create new centroids by taking the mean of all of the data points assigned to each centroid.\n",
        "\n",
        "The algorithm stops when the difference between the old and the new centroids is lower than a given threshold value. The animation below illustrates how k-means works:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEm2kuyE6LTY"
      },
      "source": [
        "![](https://images.ctfassets.net/c7lxnbtvvcxm/4IoRa5fm2Xo9qSSFYYMm9G/85146be0c66bdebddbdf1737f9e6e5b9/kmeans.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbC93-Ym66B3"
      },
      "source": [
        "In mathematical terms, the algorithm above works as an optimization process. From the optimization point of view, k-means tries to minimize its loss function. The loss function of the k-means algorithm is called the inertia, and the k-means algorithm tries to find the means (centroids) that minimize the inertia.\n",
        "\n",
        "The graph above and to the right illustrates how effective the elbow method can be when using inertia to identify the optimal number of clusters that can be used in k-means. Simply put, when the inertia begins to decrease linearly (the point where an \"elbow\" is formed), this is the optimal number of clusters. If we recall linear regression, this description for inertia is familiar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Evaluating clusters\n",
        "We now have experience clustering data with k-means, but a key question remains: what is the right value for k? To get to an answer to that question—and some others—we'll learn how to evaluate the performance of a clustering model.\n",
        "\n",
        "**Key Terms**\n",
        "\n",
        "- **Ground truth:**\n",
        "    Some existing data-point group memberships that you can use to check your clusters against\n",
        "- **Contingency table:**\n",
        "    A tabulation of the number of observations in each class against the clusters that are assigned to them\n",
        "\n",
        "Because this is unsupervised learning, we'll evaluate performance differently than with supervised learning models. As we'll see, performance measures will help us understand what a good value for $k$ is. It will also allow us to compare the performance of different clustering algorithms for the same value of $k$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Hierarchical clustering\n",
        "\n",
        "**Key Terms**\n",
        "\n",
        "- **Hierarchical clustering:**\n",
        "    A hard clustering technique that aims to build a treelike hierarchy of clusters within the data\n",
        "- **Dendrogram:**\n",
        "    A treelike hierarchical representation of clusters\n",
        "- **Bottom-up clustering:**\n",
        "    Also called agglomerative clustering, a hierarchical clustering approach that starts with the individual observations as stand-alone clusters\n",
        "- **Top-down clustering:**\n",
        "    Also called divisive clustering, an approach to hierarchical clustering that starts with a single cluster and then repeatedly divides clusters until all observations are clustered as stand-alone clusters\n",
        "\n",
        "Remember that with the k-means algorithm, we need to specify the number of clusters before we run the algorithm. However, specifying the number of clusters isn't a trivial task; more often than not, we need to experiment with alternatives.\n",
        "\n",
        "In contrast, in hierarchical clustering, we don't specify the cluster number a priori. Instead, hierarchical clustering algorithms work by combining the clusters in a hierarchical manner. So, these algorithms produce different clustering levels in a hierarchy of increasing k-values, and we choose the best one according to our needs.\n",
        "\n",
        "The visualization below demonstrates how hierarchical clustering combines data points to produce clusters. The algorithm uses a bottom-up approach; it first combines single observations one by one, and then combines the smaller clusters into bigger ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://images.ctfassets.net/c7lxnbtvvcxm/1fM2zErw0tTdkGc2sWxJ11/c05f2d68a0deeaa026cd34f408a50ab7/hierarchical_clustering.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# The DBSCAN approach to clustering\n",
        "\n",
        "Another popular clustering method: density-based spatial clustering of applications with noise (DBSCAN) finds clusters of any arbitrary shape. K-means and hierarchical clustering are good at finding circular (or convex) clusters, which makes them great tools for identifying well-separated clusters. But, unfortunately, they aren't good at identifying clusters that aren't well separated or that have nonconvex shapes such as rings inside rings, as pictured here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://images.ctfassets.net/c7lxnbtvvcxm/LX2OXExIWehp4bkdUZiBJ/1d2255f2b8b94027e4a81bf32344ee30/dbscan_intro.1.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's how DBSCAN compares to k-means and hierarchical clustering:\n",
        "\n",
        "* DBSCAN doesn't require us to specify the number of clusters.\n",
        "* DBSCAN can find clusters of any shape.\n",
        "* DBSCAN is highly efficient and scales better than k-means and hierarchical clustering.\n",
        "* DBSCAN can also identify outliers in the data, which means that it isn't sensitive to outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Clustering with Gaussian mixture models\n",
        "\n",
        "Hard clustering algorithms are algorithms that assign observations to a single cluster. There is also another category of clustering algorithms: soft clustering. In soft clustering, each observation is assigned to several clusters with associated probabilities. One such algorithm is called Gaussian mixture models. \n",
        "\n",
        "**Key Terms**\n",
        "\n",
        "- **Normal distribution:**\n",
        "    Also called a Gaussian distribution, a probability distribution in which most values cluster in the center of the range, with the rest tapering off symmetrically to the left and the right\n",
        "\n",
        "The Gaussian mixture models (GMM) algorithm belongs to a general class of probabilistic clustering algorithms. The main advantages of GMM are as follows:\n",
        "\n",
        "- It's a soft clustering algorithm. So, we can assess the confidence of the cluster assignments by investigating the probabilities.\n",
        "- It doesn't assume anything about the geometry of the clusters, unlike k-means. So, it can also tackle nonlinear geometries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Dimensionality reduction with PCA\n",
        "\n",
        "We've learned about several clustering algorithms. Clustering is an important task in unsupervised learning, and we've learned how to use various methods to discover clusters in data. Now, we'll start exploring dimensionality reduction techniques. This is another important set of algorithms in unsupervised learning. One such dimensionality reduction method is known as principal components analysis (PCA)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Dimensionality reduction with t-SNE\n",
        "\n",
        "Previously, we saw that PCA is an easy-to-use and useful dimensionality reduction algorithm that preserves dissimilarities in the data. However, PCA is relatively weak in retaining the local similarities in the low-dimensional representation. There is another dimensionality reduction algorithm: t-distributed stochastic neighbor embedding (t-SNE). This algorithm is very effective, especially in preserving the local similarities in the data.\n",
        "\n",
        "PCA is a relatively old method; it was discovered in 1933. In contrast, t-SNE is a very recent algorithm; it was proposed by Laurens van der Maaten and Geoffrey Hinton (one of the pioneers of artificial intelligence and deep learning) in 2008. As we'll see below, t-SNE is an iterative algorithm. After a sufficient number of iterations, it produces lower-dimensional representations. This is visualized in the animation below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://images.ctfassets.net/c7lxnbtvvcxm/6YHFMQUKubiIKktlaKxHPi/edc22585677f0a7022d423cc685fb218/t_sne.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Dimensionality reduction with UMAP\n",
        "\n",
        "We previously learned about t-SNE, a very successful dimensionality reduction algorithm. Since the t-SNE algorithm was proposed in 2008, many data science practitioners have welcomed it; t-SNE has become especially popular for exploratory data analysis tasks. But, as mentioned before, running t-SNE on large datasets requires lots of time and computer resources. This is cumbersome if we're doing research and prototyping, where we need to try many different things as fast as possible. We'll now learn about an alternative to t-SNE: uniform manifold approximation and projection (UMAP).\n",
        "\n",
        "UMAP's success in producing low-dimensional representations is on par with that of t-SNE, but UMAP runs much faster than t-SNE. This speed makes it a good choice, particularly for researching and prototyping. UMAP is also suitable for feature engineering when we need to reduce the dimensions in our data; in contrast, the t-SNE algorithm again suffers from long computational time when applied on very high-dimensional data.\n",
        "\n",
        "## What is UMAP?\n",
        "\n",
        "Uniform manifold approximation and projection (UMAP) is a general-purpose dimensionality reduction technique that can be used for visualization, similar to t-SNE. It can also be used for general nonlinear dimension reduction. The original paper that proposes the algorithm describes it as follows:\n",
        "\n",
        "##### The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.\n",
        "Source: [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://arxiv.org/abs/1802.03426)\n",
        "\n",
        "For more information on UMAP, see the web page [How UMAP works](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html).\n",
        "\n",
        "UMAP is an even newer algorithm than t-SNE. It was proposed in 2018—so it is a very recent work on dimensionality reduction. This should give us an idea of how active the research related to unsupervised learning is. UMAP builds upon the insights of t-SNE and offers some improvements. So understanding the differences between UMAP and t-SNE can shed some light on what UMAP achieves.\n",
        "\n",
        "## Differences between t-SNE and UMAP\n",
        "\n",
        "Compared to t-SNE, UMAP offers three major benefits:\n",
        "\n",
        "- Probably the most important improvement that UMAP brings to the table is that it's faster than t-SNE. The slowness of applying t-SNE on large datasets is a serious concern for those who lack the appropriate time and computing resources. UMAP's speed makes it the default choice when it comes to visualization of high-dimensional complex data, like image, text, and audio. That being said, keep in mind that UMAP is still slower than PCA.\n",
        "\n",
        "- In practice, t-SNE doesn't have much use apart from visualization. However, UMAP is a general-purpose dimensionality reduction algorithm that can also be used for feature engineering when a need for reducing the dimensions arises. We can use UMAP wherever we can use PCA.\n",
        "\n",
        "- When we were learning about t-SNE, it was mentioned that t-SNE preserves the local similarity as well as the global structure. But UMAP actually captures global structure better than t-SNE, while its success in preserving the local similarity is on par with t-SNE's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPMpYOS8A-Iw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
