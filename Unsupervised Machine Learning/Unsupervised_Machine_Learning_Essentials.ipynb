{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a23r3z1lUJY0"
      },
      "source": [
        "# Unsupervised Machine Learning\n",
        "\n",
        "Supervised learning focuses on creating algorithms that learn from labeled data. In unsupervised learning, we'll run algorithms on unlabeled data to derive useful patterns.\n",
        "\n",
        "**Key Terms**\n",
        "\n",
        "- **The curse of dimensionality:**\n",
        "    The exponential increase in the number of combinations for a linear increase in the number of dimensions\n",
        "- **Hard clustering:**\n",
        "    A type of clustering in which each data point is assigned to only one cluster\n",
        "- **Soft clustering:**\n",
        "    A type of clustering in which each data point is assigned a degree of membership for every cluster\n",
        "\n",
        "Unsupervised learning can be challenging, not because the algorithms are too difficult to grasp, but because the problems that unsupervised learning addresses are hard to formalize and evaluate. Most of human reasoning and learning operates in an unsupervised manner. In this section, we'll step into the exciting domain of unsupervised learning by exploring two main problems that it needs to tackle:\n",
        "\n",
        "1. Clustering\n",
        "2. Dimensionality reduction\n",
        "\n",
        "Although unsupervised learning includes other interesting topics, like anomaly detection, we'll focus on these two subjects.\n",
        "\n",
        "## What is clustering?\n",
        "\n",
        "Clustering is all about organizing individual examples or observations into groups, such that individuals in each group resemble fellow group members more than they do nongroup members. Clustering inherently requires the ability to measure similarity. For example, say that a management team gives us a customer dataset for their company. The management team asks us to help them organize customers into different groups in order to expedite customer service and drive reorganization strategies for the customer service department.\n",
        "\n",
        "Where to start? There's no need to overthink things, and we could start with a relatively straightforward grouping. We could separate the customers into different groups with respect to their city of origin, gender, income level, or all three of these variables.\n",
        "\n",
        "Grouping customers with respect to a single variable is easy. For example, we can separate the customers into two groups with respect to income."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B9B0OOnAwfj"
      },
      "source": [
        "![](https://images.ctfassets.net/c7lxnbtvvcxm/3JGWY79dK1wcyxBkm4Jf3F/6ff0a762ec63a393e761c32b3e4fa91d/customers.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2gz5pXS9CGV"
      },
      "source": [
        "From the graph above, we could assign the customers that lie above the red line to the high-income group and assign the rest to the low-income group. But what if we have hundreds of variables in your dataset, and all of them seem to be useful for the customer service efficiency? Assume, for example, that each of the hundred variables has two categories. If we group the customers using the technique above, we'll come up with $2^{100}$ groups—that's $1,267,650,600,228,229,401,496,703,205,376$ different groups!\n",
        "\n",
        "Clustering algorithms solve this problem by grouping observations (in this example, the customers) by taking into account all the variables at hand (in this example, 100 variables). Mathematically, the grouping is done in a high-dimensional space (in this example, 100 dimensions).\n",
        "\n",
        "Increasing data dimensionality doesn't require an increasing number of groups. Instead, clustering algorithms allow you to create an arbitrary number of groups. This means that we can apply clustering techniques to our customer data and ask the algorithm to come up with just four groups if our company plans to establish four different units inside the customer service department.\n",
        "\n",
        "## How many clusters?\n",
        "\n",
        "Many clustering algorithms require that we specify a desired number of groups as an input parameter.\n",
        "\n",
        "Take a look at an example. Imagine that we have a customer dataset with two variables: age and income. How many clusters do we think are appropriate?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zh4Vg3i9iO_"
      },
      "source": [
        "![](https://images.ctfassets.net/c7lxnbtvvcxm/1zj5z9S81bC8REfHygLFGl/9420f8f53236b88c87bdb2be7ae74c76/how_to_cluster_v2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gl20cYf9nU3"
      },
      "source": [
        "The graphs above show three ways that we could cluster this data:\n",
        "\n",
        "1.   Young versus old\n",
        "2.   Low-income versus high-income\n",
        "3.   Young and low-income versus young and high-income versus old and    low-income versus old and high-income\n",
        "\n",
        "Each of these clusterings may have its own advantages and disadvantages.\n",
        "\n",
        "In general, as the number of clusters increases, so does the similarity between the individuals within a cluster. This can be good to a point, but we don't want to end up with a large number of overfit clusters. Keep in mind that the limit case for clustering is that each individual case is its own cluster (which is really to say, there are no clusters). Just because an algorithm can find a certain number of clusters doesn't mean that those clusters are inherently meaningful.\n",
        "\n",
        "This gives us a hint as to why clustering is hard. In many cases, there isn't a single correct number of clusters. As we'll see in future analysis, for most clustering algorithms, the number of clusters is a hyperparameter that we need to tune.\n",
        "\n",
        "## Types of clustering\n",
        "\n",
        "**Cluster:** A set of data points that have been determined to be more similar to one another than to other data points\n",
        "\n",
        "There are two types of clustering:\n",
        "\n",
        "* In hard clustering, each data point is assigned to only one cluster.\n",
        "* In soft clustering, each data point is assigned a degree of membership for every cluster.\n",
        "\n",
        "In both types of clustering, a cluster is a set of data points that have been determined to be more similar to one another than to other data points. Depending on the clustering method that we use, we'll use different measures of similarity to analyze clusters.\n",
        "\n",
        "## What is dimensionality reduction?\n",
        "\n",
        "**Dimensionality reduction:** The process of reducing a higher-dimension dataset to a lower-dimension one, while striving to maintain as much information as possible\n",
        "\n",
        "Unsupervised learning often requires *dimensionality reduction*. In this context, *dimensions* refers to the number of features (variables) in the dataset.\n",
        "\n",
        "As a general rule, high-dimensional spaces contain at least as much information as low-dimensional spaces. That is, a dataset contains more information than a variant of that same dataset where the number of variables is reduced. This is why all dimensionality reduction techniques try to retain as much information as possible.\n",
        "\n",
        "To give us a sense of how we can represent a high-dimensional observation in a lower-dimensional space, the image below gives a familiar example. Take a look at these lower-dimensional representations of a three-dimensional cube:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13iQ8w4c-1bN"
      },
      "source": [
        "![](https://images.ctfassets.net/c7lxnbtvvcxm/3TXfSKZq4enQq2AEEvI57F/b1d4ff6aeca64ba6a1527e875ea463df/dimensions.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8qrS90j-3B8"
      },
      "source": [
        "In three-dimensional space, the cube has dimensions on the x-, y-, and z-axes. To represent this cube in two-dimensional space, we can use a square with two dimensions on the x- and y-axes. Similarly, we can represent a cube in one-dimensional space as a straight line on the x-axis.\n",
        "\n",
        "In this example, we retain quite a lot of information about the cube—even with a straight line. This is because the magnitudes of all the dimensions of a cube are the same. Now, think about a more complex three-dimensional object like a prism. In this case, we inevitably lose more information when we represent it in two- or one-dimensional space.\n",
        "\n",
        "Now, consider why dimensionality reduction is important. Say that we have a dataset that comprises millions of observations and thousands of features. In this case, dimensionality reduction can help with the following challenges:\n",
        "\n",
        "* Conducting exploratory data analysis on thousands of features is quite difficult and time consuming. It may take days, weeks, or even months to fully understand the relationships between the features.\n",
        "\n",
        "* Visualizations are one of the easiest ways to grasp data intuitively. But how can we visualize data in more than three dimensions?\n",
        "\n",
        "* Some machine learning algorithms suffer from the curse of dimensionality; as the number of features increases, the number of operations increases exponentially. Given a sufficient number of dimensions and records, algorithms can easily take days, weeks, or even months to run.\n",
        "\n",
        "* Last but not least, the more features we have, the more data we need to use to train our algorithms. Additional data collection is expensive and often not possible.\n",
        "\n",
        "This is why data scientists look to reduce dimensionality.\n",
        "\n",
        "We're already familiar with one approach to dimensionality reduction, principal component analysis (PCA), but there are many others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nPui619A9om"
      },
      "source": [
        "---\n",
        "# $K$-means\n",
        "\n",
        "We'll begin our exploration of clustering algorithms. Clustering is all about organizing individual examples or observations into groups, such that individuals in each group resemble fellow group members more than they do nongroup members. This requires the ability to measure similarity. We'll start by exploring the go-to, off-the-shelf algorithm for clustering data: the $k$-means algorithm.\n",
        "\n",
        "**Key Terms**\n",
        "\n",
        "- **Centroids:**\n",
        "    The best $k$ points that form the centers of each of the $k$ clusters in $k$-means\n",
        "- **Inertia:**\n",
        "    The loss function of the $k$-means algorithm\n",
        "\n",
        "## The $k$-means algorithm\n",
        "\n",
        "$K$-means is an iterative algorithm that eventually converges on a solution. The basic idea in $k$-means is to find the best $k$ points that form the centers of each of the $k$ clusters. These points are called centroids. The algorithm begins by choosing $k$ observations at random and making these observations the initial centroids. Then it iterates over the following two steps:\n",
        "\n",
        "1. Assign each data point to its nearest centroid.\n",
        "\n",
        "2. Create new centroids by taking the mean of all of the data points assigned to each centroid.\n",
        "\n",
        "The algorithm stops when the difference between the old and the new centroids is lower than a given threshold value. The animation below illustrates how $k$-means works:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEm2kuyE6LTY"
      },
      "source": [
        "![](https://images.ctfassets.net/c7lxnbtvvcxm/4IoRa5fm2Xo9qSSFYYMm9G/85146be0c66bdebddbdf1737f9e6e5b9/kmeans.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbC93-Ym66B3"
      },
      "source": [
        "In mathematical terms, the algorithm above works as an optimization process. From the optimization point of view, $k$-means tries to minimize its loss function. The loss function of the $k$-means algorithm is called the inertia, and the $k$-means algorithm tries to find the means (centroids) that minimize the inertia.\n",
        "\n",
        "The graph above and to the right illustrates how effective the elbow method can be when using inertia to identify the optimal number of clusters that can be used in $k$-means. Simply put, when the inertia begins to decrease linearly (the point where an \"elbow\" is formed), this is the optimal number of clusters. If we recall linear regression, this description for inertia is familiar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPMpYOS8A-Iw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
